{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "463b64e7-1433-4e24-865a-5282d58612f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torchinfo import summary\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from data_model import create_dataloaders\n",
    "from train_eval import train_epoch, eval_epoch\n",
    "from utils import accuracy, set_seeds\n",
    "from LSTM_model import ArtikelLSTM\n",
    "from Transformer_model import ArtikelTransformer\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.set_default_device(device)\n",
    "\n",
    "gender_to_idx = {\"masculine\": 0, \"feminine\": 1, \"neutral\": 2}\n",
    "artikel_to_idx = {\"der\": 0, \"die\": 1, \"das\": 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36be54ff-796d-4f99-b606-cf824da68592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "EMBED_DIM = 32\n",
    "HIDDEN_DIM = 256\n",
    "DROPOUT = 0.5\n",
    "LAYERS_NUM = 1\n",
    "\n",
    "OPTIMIZER = \"Adam\"\n",
    "LEARNING_RATE = 0.00005 * BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbb38f10-bf5a-479d-a028-6f0c66329f35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "set_seeds(seed=42)\n",
    "run_name = f\"LSTM-lr{LEARNING_RATE}-batch{BATCH_SIZE}-opt{OPTIMIZER}-drop{DROPOUT}-emb{EMBED_DIM}-hid{HIDDEN_DIM}-lay{LAYERS_NUM}-hot-full-scheduler\"\n",
    "writer = SummaryWriter(log_dir=f'runs/{run_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c789b1dd-491e-49ac-bfb3-27d7c5efd323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words with der: 113935, words with die: 143243, words with das: 69125\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "train_dataloader, test_dataloader, char_to_idx = create_dataloaders(file_path=\"words_big.txt\", data_fraction=1, test_size=0.15, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7830905d-44cb-479b-a428-a1a0c802c21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================================================================================\n",
      "Layer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable            Mult-Adds\n",
      "============================================================================================================================================\n",
      "ArtikelLSTM (ArtikelLSTM)                [1, 10]              [1, 3]               992                  True                 --\n",
      "├─LSTM (lstm)                            [1, 10, 32]          [1, 10, 512]         593,920              True                 5,939,200\n",
      "│    └─weight_ih_l0                                                                ├─32,768\n",
      "│    └─weight_hh_l0                                                                ├─262,144\n",
      "│    └─bias_ih_l0                                                                  ├─1,024\n",
      "│    └─bias_hh_l0                                                                  ├─1,024\n",
      "│    └─weight_ih_l0_reverse                                                        ├─32,768\n",
      "│    └─weight_hh_l0_reverse                                                        ├─262,144\n",
      "│    └─bias_ih_l0_reverse                                                          ├─1,024\n",
      "│    └─bias_hh_l0_reverse                                                          └─1,024\n",
      "├─Sequential (indices_fc)                [1, 512]             [1, 3]               --                   True                 --\n",
      "│    └─0.weight                                                                    ├─1,536\n",
      "│    └─0.bias                                                                      └─3\n",
      "│    └─Linear (0)                        [1, 512]             [1, 3]               1,539                True                 1,539\n",
      "│    │    └─weight                                                                 ├─1,536\n",
      "│    │    └─bias                                                                   └─3\n",
      "============================================================================================================================================\n",
      "Total params: 596,451\n",
      "Trainable params: 596,451\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 5.94\n",
      "============================================================================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.04\n",
      "Params size (MB): 2.38\n",
      "Estimated Total Size (MB): 2.42\n",
      "============================================================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tomek\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "model = ArtikelLSTM(vocab_size=31, embedding_dim=EMBED_DIM, hidden_dim=HIDDEN_DIM, dropout=DROPOUT, num_layers=LAYERS_NUM)\n",
    "#model = ArtikelTransformer(vocab_size=31, embedding_dim=8, num_heads=4, hidden_dim=16, num_layers=4)\n",
    "model.to(device)\n",
    "\n",
    "_ = summary(model, input_size=(1, 10), dtypes=[torch.long], col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\", \"mult_adds\"], col_width=20, row_settings=[\"var_names\"], verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4555ad35-cabf-4d38-ae7e-6aed3a01f204",
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPTIMIZER == \"Adam\":\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "elif OPTIMIZER == \"SGD\":\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "max_accuracy = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f03255d8-c45f-44f2-a212-ff25977b2369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train_loss: 0.4106, Test_loss: 0.1969, Accuracy: 93.2555\n",
      "Epoch 2, Train_loss: 0.1646, Test_loss: 0.1561, Accuracy: 94.9023\n",
      "Epoch 3, Train_loss: 0.1296, Test_loss: 0.1462, Accuracy: 95.1351\n",
      "Epoch 4, Train_loss: 0.1092, Test_loss: 0.1421, Accuracy: 95.3245\n",
      "Epoch 5, Train_loss: 0.0945, Test_loss: 0.1423, Accuracy: 95.3020\n",
      "Epoch 6, Train_loss: 0.0821, Test_loss: 0.1572, Accuracy: 95.2291\n",
      "Epoch 7, Train_loss: 0.0713, Test_loss: 0.1644, Accuracy: 95.2816\n",
      "Epoch 8, Train_loss: 0.0622, Test_loss: 0.1686, Accuracy: 95.3700\n",
      "Epoch 9, Train_loss: 0.0526, Test_loss: 0.1779, Accuracy: 95.3680\n",
      "Epoch 10, Train_loss: 0.0455, Test_loss: 0.1903, Accuracy: 95.4196\n",
      "Epoch 11, Train_loss: 0.0389, Test_loss: 0.2058, Accuracy: 95.4395\n",
      "Epoch 12, Train_loss: 0.0327, Test_loss: 0.2207, Accuracy: 95.4129\n",
      "Epoch 13, Train_loss: 0.0276, Test_loss: 0.2303, Accuracy: 95.2872\n",
      "Epoch 14, Train_loss: 0.0230, Test_loss: 0.2533, Accuracy: 95.3190\n",
      "Epoch 15, Train_loss: 0.0189, Test_loss: 0.2772, Accuracy: 95.2842\n",
      "Epoch 16, Train_loss: 0.0160, Test_loss: 0.2730, Accuracy: 95.2842\n",
      "Epoch 17, Train_loss: 0.0132, Test_loss: 0.3001, Accuracy: 95.2868\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#add_text(\"\")\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m20\u001b[39m):  \u001b[38;5;66;03m# Number of epochs\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     test_loss, test_accuracy \u001b[38;5;241m=\u001b[39m eval_epoch(model, test_dataloader, loss_fn, device)\n\u001b[0;32m      7\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mD:\\Projects\\german_words\\train_eval.py:9\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, train_dataloader, optimizer, loss_fn, device)\u001b[0m\n\u001b[0;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 9\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mwords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwords\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mD:\\Projects\\german_words\\data_model.py:19\u001b[0m, in \u001b[0;36mGermanNounDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Convert word to character indices\u001b[39;00m\n\u001b[0;32m     17\u001b[0m word_indices \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchar_to_idx[char] \u001b[38;5;28;01mfor\u001b[39;00m char \u001b[38;5;129;01min\u001b[39;00m word]\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m, index\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\_device.py:78\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[1;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     77\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m---> 78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "#add_text(\"\")\n",
    "\n",
    "for epoch in range(20):  # Number of epochs\n",
    "    train_loss = train_epoch(model, train_dataloader, optimizer, loss_fn, device)\n",
    "    test_loss, test_accuracy = eval_epoch(model, test_dataloader, loss_fn, device)\n",
    "    scheduler.step()\n",
    "\n",
    "    writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "    writer.add_scalar('Loss/test', test_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/train', test_accuracy, epoch)\n",
    "    \n",
    "    train_losses.append(train_loss.cpu().detach().numpy())\n",
    "    test_losses.append(test_loss.cpu())\n",
    "    test_accuracies.append(test_accuracy)\n",
    "\n",
    "    if test_accuracy > max_accuracy:\n",
    "        max_accuracy = test_accuracy\n",
    "        torch.save(model.state_dict(), f'models/{run_name}' + f'-acc{test_accuracy:.2f}.pth')\n",
    "        \n",
    "\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Train_loss: {train_loss.item():.4f}, Test_loss: {test_loss.item():.4f}, Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2246c00e-f393-4f14-965c-ae6ab49f06a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_losses, label=\"test\")\n",
    "plt.plot(train_losses, label=\"train\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587e8d1c-287b-4504-9c14-31280941af92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(train_accuracies, label=\"train\")\n",
    "plt.plot(test_accuracies, label=\"test\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb806bbf-0f1b-451a-9781-6229db6722c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, word, char_to_idx):\n",
    "    model.eval()\n",
    "    word_indices = [char_to_idx[char] for char in word.lower()]\n",
    "    word_tensor = torch.tensor(word_indices, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(word_tensor)\n",
    "\n",
    "        idx = torch.argmax(output, dim=1).item()\n",
    "\n",
    "    idx_to_artikel = {v: k for k, v in artikel_to_idx.items()}\n",
    "\n",
    "    return idx_to_artikel[idx]\n",
    "\n",
    "# Example usage\n",
    "print(predict(model, \"Auto\", char_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c0cf2c-e7e1-4fea-9d69-8cb4b0a9d18c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
