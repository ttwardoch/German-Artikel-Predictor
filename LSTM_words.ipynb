{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "463b64e7-1433-4e24-865a-5282d58612f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torchinfo import summary\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from data_model import create_dataloaders\n",
    "from train_eval import train_epoch, eval_epoch\n",
    "from utils import accuracy, set_seeds\n",
    "from LSTM_model import ArtikelLSTM\n",
    "from Transformer_model import ArtikelTransformer\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.set_default_device(device)\n",
    "\n",
    "gender_to_idx = {\"masculine\": 0, \"feminine\": 1, \"neutral\": 2}\n",
    "artikel_to_idx = {\"der\": 0, \"die\": 1, \"das\": 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36be54ff-796d-4f99-b606-cf824da68592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "EMBED_DIM = 32\n",
    "HIDDEN_DIM = 256\n",
    "DROPOUT = 0.5\n",
    "LAYERS_NUM = 1\n",
    "\n",
    "OPTIMIZER = \"Adam\"\n",
    "LEARNING_RATE = 0.00005 * BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbb38f10-bf5a-479d-a028-6f0c66329f35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "set_seeds(seed=42)\n",
    "run_name = f\"LSTM-lr{LEARNING_RATE}-batch{BATCH_SIZE}-opt{OPTIMIZER}-drop{DROPOUT}-emb{EMBED_DIM}-hid{HIDDEN_DIM}-lay{LAYERS_NUM}-hot-full-final\"\n",
    "writer = SummaryWriter(log_dir=f'runs/{run_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c789b1dd-491e-49ac-bfb3-27d7c5efd323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words with der: 113935, words with die: 143243, words with das: 69125\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "train_dataloader, test_dataloader, char_to_idx = create_dataloaders(file_path=\"words_big.txt\", data_fraction=1, test_size=0.15, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7830905d-44cb-479b-a428-a1a0c802c21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================================================================================\n",
      "Layer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable            Mult-Adds\n",
      "============================================================================================================================================\n",
      "ArtikelLSTM (ArtikelLSTM)                [1, 10]              [1, 3]               992                  True                 --\n",
      "├─LSTM (lstm)                            [1, 10, 32]          [1, 10, 512]         593,920              True                 5,939,200\n",
      "│    └─weight_ih_l0                                                                ├─32,768\n",
      "│    └─weight_hh_l0                                                                ├─262,144\n",
      "│    └─bias_ih_l0                                                                  ├─1,024\n",
      "│    └─bias_hh_l0                                                                  ├─1,024\n",
      "│    └─weight_ih_l0_reverse                                                        ├─32,768\n",
      "│    └─weight_hh_l0_reverse                                                        ├─262,144\n",
      "│    └─bias_ih_l0_reverse                                                          ├─1,024\n",
      "│    └─bias_hh_l0_reverse                                                          └─1,024\n",
      "├─Sequential (indices_fc)                [1, 512]             [1, 3]               --                   True                 --\n",
      "│    └─0.weight                                                                    ├─1,536\n",
      "│    └─0.bias                                                                      └─3\n",
      "│    └─Linear (0)                        [1, 512]             [1, 3]               1,539                True                 1,539\n",
      "│    │    └─weight                                                                 ├─1,536\n",
      "│    │    └─bias                                                                   └─3\n",
      "============================================================================================================================================\n",
      "Total params: 596,451\n",
      "Trainable params: 596,451\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 5.94\n",
      "============================================================================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.04\n",
      "Params size (MB): 2.38\n",
      "Estimated Total Size (MB): 2.42\n",
      "============================================================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tomek\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "model = ArtikelLSTM(vocab_size=31, embedding_dim=EMBED_DIM, hidden_dim=HIDDEN_DIM, dropout=DROPOUT, num_layers=LAYERS_NUM)\n",
    "#model = ArtikelTransformer(vocab_size=31, embedding_dim=8, num_heads=4, hidden_dim=16, num_layers=4)\n",
    "model.to(device)\n",
    "\n",
    "_ = summary(model, input_size=(1, 10), dtypes=[torch.long], col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\", \"mult_adds\"], col_width=20, row_settings=[\"var_names\"], verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4555ad35-cabf-4d38-ae7e-6aed3a01f204",
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPTIMIZER == \"Adam\":\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "elif OPTIMIZER == \"SGD\":\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "#scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "max_accuracy = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03255d8-c45f-44f2-a212-ff25977b2369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "#add_text(\"\")\n",
    "\n",
    "for epoch in range(20):  # Number of epochs\n",
    "    train_loss = train_epoch(model, train_dataloader, optimizer, loss_fn, device)\n",
    "    test_loss, test_accuracy = eval_epoch(model, test_dataloader, loss_fn, device)\n",
    "    #scheduler.step()\n",
    "\n",
    "    writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "    writer.add_scalar('Loss/test', test_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/train', test_accuracy, epoch)\n",
    "    \n",
    "    train_losses.append(train_loss.cpu().detach().numpy())\n",
    "    test_losses.append(test_loss.cpu())\n",
    "    test_accuracies.append(test_accuracy)\n",
    "\n",
    "    if test_accuracy > max_accuracy:\n",
    "        max_accuracy = test_accuracy\n",
    "        torch.save(model.state_dict(), f'models/{run_name}' + f'-acc{test_accuracy:.2f}.pth')\n",
    "        \n",
    "\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Train_loss: {train_loss.item():.4f}, Test_loss: {test_loss.item():.4f}, Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2246c00e-f393-4f14-965c-ae6ab49f06a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_losses, label=\"test\")\n",
    "plt.plot(train_losses, label=\"train\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587e8d1c-287b-4504-9c14-31280941af92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(train_accuracies, label=\"train\")\n",
    "plt.plot(test_accuracies, label=\"test\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb806bbf-0f1b-451a-9781-6229db6722c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(f'models/{run_name}' + f'-acc{max_accuracy:.2f}.pth', weights_only=True))\n",
    "\n",
    "def predict(model, word, char_to_idx):\n",
    "    model.eval()\n",
    "    word_indices = [char_to_idx[char] for char in word.lower()]\n",
    "    word_tensor = torch.tensor(word_indices, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(word_tensor)\n",
    "\n",
    "        idx = torch.argmax(output, dim=1).item()\n",
    "\n",
    "    idx_to_artikel = {v: k for k, v in artikel_to_idx.items()}\n",
    "\n",
    "    return idx_to_artikel[idx]\n",
    "\n",
    "# Example usage\n",
    "print(predict(model, \"Auto\", char_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c0cf2c-e7e1-4fea-9d69-8cb4b0a9d18c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
