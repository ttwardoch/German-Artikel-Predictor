{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "463b64e7-1433-4e24-865a-5282d58612f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "from data_model import create_dataloaders\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "  correct = torch.eq(y_true, y_pred).sum().item()\n",
    "  return correct/len(y_pred)*100\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "# Label encoding for genders and plurality\n",
    "gender_to_idx = {\"masculine\": 0, \"feminine\": 1, \"neutral\": 2}\n",
    "\n",
    "# Der die das to idx:\n",
    "artikel_to_idx = {\"der\": 0, \"die\": 1, \"das\": 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbb38f10-bf5a-479d-a028-6f0c66329f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c789b1dd-491e-49ac-bfb3-27d7c5efd323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words with der: 113935, words with die: 143243, words with das: 69125\n"
     ]
    }
   ],
   "source": [
    "train_dataloader, test_dataloader = create_dataloaders(file_path=\"words_big.txt\", data_fraction=1, test_size=0.2, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7830905d-44cb-479b-a428-a1a0c802c21a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "========================================================================================================================\n",
       "Layer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable\n",
       "========================================================================================================================\n",
       "ArtikelClassifier (ArtikelClassifier)    [1, 10]              [1, 3]               --                   True\n",
       "├─Embedding (embedding)                  [1, 10]              [1, 10, 32]          992                  True\n",
       "├─LSTM (lstm)                            [1, 10, 32]          [1, 10, 16]          2,688                True\n",
       "├─Sequential (indices_fc)                [1, 16]              [1, 3]               --                   True\n",
       "│    └─Linear (0)                        [1, 16]              [1, 3]               51                   True\n",
       "========================================================================================================================\n",
       "Total params: 3,731\n",
       "Trainable params: 3,731\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.03\n",
       "========================================================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.01\n",
       "Estimated Total Size (MB): 0.02\n",
       "========================================================================================================================"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ArtikelTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_heads, hidden_dim, num_layers):\n",
    "        super(ArtikelTransformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.pos_embedding = nn.Parameter(torch.zeros(1, 100, embedding_dim))  # Assuming max length of 100\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embedding_dim, nhead=num_heads, dim_feedforward=hidden_dim\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc_indices = nn.Linear(embedding_dim, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        pos_embedded = self.pos_embedding[:, :seq_len, :]\n",
    "        embedded = self.embedding(x) + pos_embedded\n",
    "        transformer_out = self.transformer(embedded)\n",
    "        pooled_out = transformer_out.mean(dim=1)\n",
    "        return self.fc_indices(pooled_out)\n",
    "\n",
    "class ArtikelClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, dropout, num_layers):\n",
    "        super(ArtikelClassifier, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, dropout=dropout, bidirectional=True, num_layers=num_layers)\n",
    "        \n",
    "        self.indices_fc = nn.Sequential(nn.Linear(2*hidden_dim*num_layers, 3),\n",
    "                                       )\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        _, (hidden, _) = self.lstm(embedded)\n",
    "        hidden = hidden.squeeze(0)\n",
    "        hidden = torch.cat(tuple([hidden[i] for i in range(2*self.num_layers)]), dim=1)\n",
    "        #hidden = hidden.squeeze(0)\n",
    "\n",
    "        indices_output = self.indices_fc(hidden)\n",
    "\n",
    "        return indices_output\n",
    "\n",
    "# Initialize the model\n",
    "vocab_size = len(char_to_idx)\n",
    "embedding_dim = 8\n",
    "hidden_dim = 16\n",
    "num_filters = 16\n",
    "num_heads = 8\n",
    "num_layers = 4\n",
    "model = ArtikelClassifier(vocab_size, embedding_dim=32, hidden_dim=8, dropout=0.8, num_layers=1)\n",
    "#model = ArtikelTransformer(vocab_size, embedding_dim, num_heads, hidden_dim, num_layers).to(device)\n",
    "model.to(device)\n",
    "\n",
    "summary(model, input_size=(1, 10), dtypes=[torch.long], col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"], col_width=20, row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4555ad35-cabf-4d38-ae7e-6aed3a01f204",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "test_accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f03255d8-c45f-44f2-a212-ff25977b2369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train_loss: 0.7404, Test_loss: 0.6353, Accuracy: 74.6152\n",
      "Epoch 2, Train_loss: 0.5931, Test_loss: 0.5564, Accuracy: 78.5297\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# Combine the losses\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     20\u001b[0m train_loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_dataloader)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:260\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    251\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    252\u001b[0m     (inputs,)\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, (torch\u001b[38;5;241m.\u001b[39mTensor, graph\u001b[38;5;241m.\u001b[39mGradientEdge))\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m()\n\u001b[0;32m    257\u001b[0m )\n\u001b[0;32m    259\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[38;5;28mlen\u001b[39m(tensors))\n\u001b[1;32m--> 260\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m \u001b[43m_make_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:143\u001b[0m, in \u001b[0;36m_make_grads\u001b[1;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[0;32m    137\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    138\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for real scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    139\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    140\u001b[0m         )\n\u001b[0;32m    141\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg)\n\u001b[0;32m    142\u001b[0m     new_grads\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 143\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreserve_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    144\u001b[0m     )\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    146\u001b[0m     new_grads\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(5):  # Number of epochs\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    for words, indices in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        indices_output = model(words.to(device))\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = F.cross_entropy(indices_output, indices.to(device))\n",
    "        train_loss += loss\n",
    "\n",
    "        # Combine the losses\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss /= len(train_dataloader)\n",
    "    train_losses.append(train_loss.cpu().detach().numpy())\n",
    "\n",
    "    test_loss = 0\n",
    "    test_accuracy = 0\n",
    "    with torch.inference_mode():\n",
    "        for words, indices in test_dataloader:\n",
    "            # Forward pass\n",
    "            indices_output = model(words.to(device))\n",
    "    \n",
    "            # Calculate loss\n",
    "            test_loss += F.cross_entropy(indices_output, indices.to(device))\n",
    "            test_accuracy += accuracy(indices, indices_output.argmax(dim=1).cpu())\n",
    "\n",
    "        test_loss /= len(test_dataloader)\n",
    "        test_losses.append(test_loss.cpu())\n",
    "\n",
    "        test_accuracy /= len(test_dataloader)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Train_loss: {train_loss.item():.4f}, Test_loss: {test_loss.item():.4f}, Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2246c00e-f393-4f14-965c-ae6ab49f06a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_losses, label=\"test\")\n",
    "plt.plot(train_losses, label=\"train\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587e8d1c-287b-4504-9c14-31280941af92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(train_accuracies, label=\"train\")\n",
    "plt.plot(test_accuracies, label=\"test\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb806bbf-0f1b-451a-9781-6229db6722c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, word, char_to_idx):\n",
    "    model.eval()\n",
    "    word_indices = [char_to_idx[char] for char in word.lower()]\n",
    "    word_tensor = torch.tensor(word_indices, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(word_tensor)\n",
    "\n",
    "        idx = torch.argmax(output, dim=1).item()\n",
    "\n",
    "    idx_to_artikel = {v: k for k, v in artikel_to_idx.items()}\n",
    "\n",
    "    return idx_to_artikel[idx]\n",
    "\n",
    "# Example usage\n",
    "print(predict(model, \"Popo\", char_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c0cf2c-e7e1-4fea-9d69-8cb4b0a9d18c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
