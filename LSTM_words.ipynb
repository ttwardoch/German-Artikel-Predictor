{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "463b64e7-1433-4e24-865a-5282d58612f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "from data_model import create_dataloaders\n",
    "from utils import accuracy, set_seeds\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "# Label encoding for genders and plurality\n",
    "gender_to_idx = {\"masculine\": 0, \"feminine\": 1, \"neutral\": 2}\n",
    "\n",
    "# Der die das to idx:\n",
    "artikel_to_idx = {\"der\": 0, \"die\": 1, \"das\": 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bbb38f10-bf5a-479d-a028-6f0c66329f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c789b1dd-491e-49ac-bfb3-27d7c5efd323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words with der: 113935, words with die: 143243, words with das: 69125\n"
     ]
    }
   ],
   "source": [
    "train_dataloader, test_dataloader = create_dataloaders(file_path=\"words_big.txt\", data_fraction=0.2, test_size=0.2, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7830905d-44cb-479b-a428-a1a0c802c21a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "========================================================================================================================\n",
       "Layer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable\n",
       "========================================================================================================================\n",
       "ArtikelClassifier (ArtikelClassifier)    [1, 10]              [1, 3]               --                   True\n",
       "├─Embedding (embedding)                  [1, 10]              [1, 10, 32]          992                  True\n",
       "├─LSTM (lstm)                            [1, 10, 32]          [1, 10, 16]          2,688                True\n",
       "├─Sequential (indices_fc)                [1, 16]              [1, 3]               --                   True\n",
       "│    └─Linear (0)                        [1, 16]              [1, 3]               51                   True\n",
       "========================================================================================================================\n",
       "Total params: 3,731\n",
       "Trainable params: 3,731\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.03\n",
       "========================================================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.01\n",
       "Estimated Total Size (MB): 0.02\n",
       "========================================================================================================================"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ArtikelTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_heads, hidden_dim, num_layers):\n",
    "        super(ArtikelTransformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.pos_embedding = nn.Parameter(torch.zeros(1, 100, embedding_dim))  # Assuming max length of 100\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embedding_dim, nhead=num_heads, dim_feedforward=hidden_dim\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc_indices = nn.Linear(embedding_dim, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        pos_embedded = self.pos_embedding[:, :seq_len, :]\n",
    "        embedded = self.embedding(x) + pos_embedded\n",
    "        transformer_out = self.transformer(embedded)\n",
    "        pooled_out = transformer_out.mean(dim=1)\n",
    "        return self.fc_indices(pooled_out)\n",
    "\n",
    "class ArtikelClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, dropout, num_layers):\n",
    "        super(ArtikelClassifier, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, dropout=dropout, bidirectional=True, num_layers=num_layers)\n",
    "        \n",
    "        self.indices_fc = nn.Sequential(nn.Linear(2*hidden_dim*num_layers, 3),\n",
    "                                       )\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        _, (hidden, _) = self.lstm(embedded)\n",
    "        hidden = hidden.squeeze(0)\n",
    "        hidden = torch.cat(tuple([hidden[i] for i in range(2*self.num_layers)]), dim=1)\n",
    "        #hidden = hidden.squeeze(0)\n",
    "\n",
    "        indices_output = self.indices_fc(hidden)\n",
    "\n",
    "        return indices_output\n",
    "\n",
    "# Initialize the model\n",
    "vocab_size = len(char_to_idx)\n",
    "embedding_dim = 8\n",
    "hidden_dim = 16\n",
    "num_filters = 16\n",
    "num_heads = 8\n",
    "num_layers = 4\n",
    "model = ArtikelClassifier(vocab_size, embedding_dim=32, hidden_dim=8, dropout=0.8, num_layers=1)\n",
    "#model = ArtikelTransformer(vocab_size, embedding_dim, num_heads, hidden_dim, num_layers).to(device)\n",
    "model.to(device)\n",
    "\n",
    "summary(model, input_size=(1, 10), dtypes=[torch.long], col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"], col_width=20, row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4555ad35-cabf-4d38-ae7e-6aed3a01f204",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "test_accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03255d8-c45f-44f2-a212-ff25977b2369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(5):  # Number of epochs\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    for words, indices in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        indices_output = model(words.to(device))\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = F.cross_entropy(indices_output, indices.to(device))\n",
    "        train_loss += loss\n",
    "\n",
    "        # Combine the losses\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss /= len(train_dataloader)\n",
    "    train_losses.append(train_loss.cpu().detach().numpy())\n",
    "\n",
    "    test_loss = 0\n",
    "    test_accuracy = 0\n",
    "    with torch.inference_mode():\n",
    "        for words, indices in test_dataloader:\n",
    "            # Forward pass\n",
    "            indices_output = model(words.to(device))\n",
    "    \n",
    "            # Calculate loss\n",
    "            test_loss += F.cross_entropy(indices_output, indices.to(device))\n",
    "            test_accuracy += accuracy(indices, indices_output.argmax(dim=1).cpu())\n",
    "\n",
    "        test_loss /= len(test_dataloader)\n",
    "        test_losses.append(test_loss.cpu())\n",
    "\n",
    "        test_accuracy /= len(test_dataloader)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Train_loss: {train_loss.item():.4f}, Test_loss: {test_loss.item():.4f}, Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2246c00e-f393-4f14-965c-ae6ab49f06a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_losses, label=\"test\")\n",
    "plt.plot(train_losses, label=\"train\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587e8d1c-287b-4504-9c14-31280941af92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(train_accuracies, label=\"train\")\n",
    "plt.plot(test_accuracies, label=\"test\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb806bbf-0f1b-451a-9781-6229db6722c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, word, char_to_idx):\n",
    "    model.eval()\n",
    "    word_indices = [char_to_idx[char] for char in word.lower()]\n",
    "    word_tensor = torch.tensor(word_indices, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(word_tensor)\n",
    "\n",
    "        idx = torch.argmax(output, dim=1).item()\n",
    "\n",
    "    idx_to_artikel = {v: k for k, v in artikel_to_idx.items()}\n",
    "\n",
    "    return idx_to_artikel[idx]\n",
    "\n",
    "# Example usage\n",
    "print(predict(model, \"Popo\", char_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c0cf2c-e7e1-4fea-9d69-8cb4b0a9d18c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
